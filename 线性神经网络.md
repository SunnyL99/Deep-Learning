# 基础优化算法
## 梯度下降
当模型没有显示解时：首先挑选一个参数的随机初始值w0，重复迭代更新w0，使其接近最优解
【梯度：使函数增加最快的方向。负梯度：使函数下降最快的方向】
学习率：沿着这个方向每次走多远。

### 学习率
步长的超参数 (hyper-parameter)，人为的指定值
选择学习率
不能太小：每一步走太小，到达下一个点需要很多步。计算梯度很贵，尽量少来计算梯度。
也不能太大：容易迈过了下降的地方，略过了下一级到了同级，使得一直震荡，没有在下降
## 小批量随机梯度下降
很少直接使用梯度下降。
因为每次计算梯度，要对整个损失函数求导，损失函数是对多有样本的平均损失。需要几分钟至数小时。
解决方法：随机采样b个样本来近似损失。b个样本损失的平均近似整个样本损失的平均。

### b批量大小
是重要的超参数
b很大时，近似很精确；b很小，不那么精确，但是容易算。
选择批量大小
不能太小：计算量太小，不适合并行来最大利用计算资源。
不能太大：内存消耗大；如果包含过多相同样本，浪费计算

## 总结
梯度下降不断沿着反梯度方向更新参数求解：不需要知道损失函数什么样，只需要不断跟着导数求
小批量随机梯度下降是深度学习默认的求解算法：有更好的，但他是最稳定最简单的
两个重要的超参数是批量大小和学习率
# Softmax回归
虽然叫回归，但是是分类问题
回归：估计一个连续值。单连续值输出；自然区间R；跟真实值的区别作为损失
分类：预测一个离散类别。多个输出；输出i是预测为第i类的置信度
### 从回归到多类分类 - 均方损失
对类别进行一位有效编码
使用均方损失训练
最大值作为预测
需要更置信的识别正确类
输出匹配概率（非负，和为1）
概率y和y-hat作为损失
交叉熵用来衡量两个概率的区别
将它作为损失
【对于分类问题，只关心正确类的预测指标，不关心不正确类的预测】
